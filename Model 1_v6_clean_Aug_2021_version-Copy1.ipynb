{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b809ca-d355-4ea4-bbfa-fec0d925069f",
   "metadata": {},
   "source": [
    "##Notes: the approach taken in this baseline model is from this 2020 article\n",
    "##by Lianne and Justin, thanks to them for sharing. They used \n",
    "##ridge regression alpha = 0.001\n",
    "\n",
    "https://www.justintodata.com/improve-sports-betting-odds-guide-in-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c205a5b-9942-4f1a-85b2-c1b6555260f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f381044-d18f-420b-aa3e-198bab2e0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "##we will try the following models on the base-line data ... just win/loss and which teams\n",
    "\n",
    "##note KNN or other clusters might be helpful group the teams in smart way ... but not now.\n",
    "#models\n",
    "\n",
    "##regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#classifiers (non-tree)\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#tree-based classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "##regression models\n",
    "lr = Ridge(alpha=0.001) \n",
    "rfr = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "regr_models = [lr, rfr, xgbr]\n",
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0)\n",
    "svc = SVC()\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=3, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier( use_label_encoder=False, num_class = [0,1])\n",
    "\n",
    "class_models= [lrc, gnb, lgr, svc, rfc, bc, gbc, xgbc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f088b3e0-25fd-4c78-9d1f-c961aa18bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"/Users/joejohns/data_bootcamp/GitHub/final_project_nhl_prediction/Data/Processed_Data/Approach_1 and_2_win_loss_and_cumul_1seas_Pisch_data/data_bet_stats_mp.csv\")\n",
    "data.drop(columns=[ 'Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9c28c6-3904-4b69-a266-1fd75ef6d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['won'] = data['won'].apply(int)\n",
    "data_playoffs = data.loc[data['playoffGame'] == 1, :].copy()  #set aside playoff games ... probably won't use them.\n",
    "data=  data.loc[data['playoffGame'] == 0, :].copy() \n",
    "\n",
    "#sorted(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7ca1f0-88c7-4b4d-9fd8-f393055b9028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20082009,\n",
       " 20092010,\n",
       " 20102011,\n",
       " 20112012,\n",
       " 20122013,\n",
       " 20132014,\n",
       " 20142015,\n",
       " 20152016,\n",
       " 20162017,\n",
       " 20172018,\n",
       " 20182019,\n",
       " 20192020]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_seasons = sorted(set(data['season']))\n",
    "all_seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9195263-13db-4291-b7a5-980c4af44d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_HA_data(X, season, list_var_names = None ):\n",
    "    X = X.loc[X['season'] == season, :].copy()\n",
    "    X_H = X.loc[X['HoA'] == 'home',:].copy()\n",
    "    X_A = X.loc[X['HoA'] == 'away',:].copy()\n",
    "    X_H['goal_difference'] = X_H['goalsFor'] - X_H['goalsAgainst']  ##note every thing is based in home data\n",
    "    X_H.reset_index(drop = True, inplace = True)\n",
    "    X_A.reset_index(drop = True, inplace = True)\n",
    "    df_visitor = pd.get_dummies(X_H['nhl_name'], dtype=np.int64)\n",
    "    df_home = pd.get_dummies(X_A['nhl_name'], dtype=np.int64)\n",
    "    df_model = df_home.sub(df_visitor) \n",
    "    df_model['date'] = X_H['date']\n",
    "    df_model['full_date'] = X_H['full_date']\n",
    "    \n",
    "    df_model['game_id'] = X_H['game_id']\n",
    "    df_model['home_id'] = X_H['team_id']\n",
    "    df_model['away_id'] = X_A['team_id'] \n",
    "    y = X_H.loc[:,['date', 'full_date','game_id', 'Open','goal_difference', 'won']].copy()   ##these are from home team perspective; 'Open' is for betting \n",
    "    return (df_model, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d2027c-ca62-4f3e-9faf-c7f1e2067059",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dic = {}\n",
    "y_dic = {}\n",
    "for sea in all_seasons:\n",
    "    X_dic[sea] = make_HA_data(data, sea)[0]\n",
    "    y_dic[sea] = make_HA_data(data, sea)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da75b0a7-0957-4d04-a4b7-cc45cf006a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for  regressors predicting wins - losses, can use this to turn output into win prediction \n",
    "\n",
    "def make_win(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >0:\n",
    "        return 1\n",
    "\n",
    "v_make_win = np.vectorize(make_win)\n",
    "\n",
    "#useage: v_make_win(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1546abb7-f86a-49c9-a79a-27a46282d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "##naive method: train on first half of season, 600 games, test on second half of season\n",
    "##with no further training\n",
    "\n",
    "def naive_test_train_regr_models(model, cut_off = 600, regr = True):\n",
    "    all_seasons2 = [sea for sea in all_seasons if sea != 20122013]#2012 is shortened season\n",
    "    total_acc = 0\n",
    "    counter = 0\n",
    "    model_name = str(model)\n",
    "    print(\"results for \", model_name)\n",
    "    print(\" \")\n",
    "    for sea in all_seasons2:\n",
    "       \n",
    "        #set teh predictor variables, :-5 does the job, would be better \n",
    "        #and safer to name the columns explcitly ... but the columns are date\n",
    "        #and so on ... no leakage worries. OK for this base line\n",
    "        \n",
    "        X = X_dic[sea].iloc[:, :-5].copy()\n",
    "        \n",
    "        #select season, remove date, etc. select target y\n",
    "        if regr == True:\n",
    "            y = y_dic[sea].loc[:, 'goal_difference'].copy()\n",
    "        else:\n",
    "            y = y_dic[sea].loc[:, 'won'].copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #carry out naive train-test split\n",
    "        y_train = y[0: cut_off].copy()\n",
    "        y_test = y[cut_off :].copy()\n",
    "        X_train = X[0: cut_off].copy()\n",
    "        X_test = X[cut_off :].copy()\n",
    "        \n",
    "        #train model, find predictions\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test) #this is regression pred on Hg - Ag\n",
    "        \n",
    "        y_pred_win = v_make_win(y_pred) #this is the pred of who wins HW =1, AW =0\n",
    "        y_test_win = v_make_win(y_test)  #this gives the correct win, loss\n",
    "        #note: if y, y_pred and y_test are already 1, 0 then v_make_win will \n",
    "        #keep them the same (<= 0 --> 0, >0 ---> 1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test_win, y_pred_win)\n",
    "        f1 = f1_score(y_test_win, y_pred_win) #, average = None)\n",
    "        \n",
    "        counter+=1\n",
    "        total_acc+= accuracy\n",
    "        \n",
    "        print(\"seaoson: \", sea)\n",
    "        print(\"acc: \", accuracy, \" f1: \", f1)\n",
    "    \n",
    "    avg_acc = total_acc/counter\n",
    "    print('avg acuracy: ', avg_acc)\n",
    "    print(\" \")    \n",
    "        #evaluate_regression(y_test, y_pred)\n",
    "        #evaluate_binary_classification(y_test_win, y_pred_win\n",
    "       \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f459d71a-7e77-49a9-a195-f8e5df10842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for  Ridge(alpha=0.001)\n",
      " \n",
      "seaoson:  20082009\n",
      "acc:  0.5351043643263758  f1:  0.6067415730337079\n",
      "seaoson:  20092010\n",
      "acc:  0.5587121212121212  f1:  0.616144975288303\n",
      "seaoson:  20102011\n",
      "acc:  0.5075471698113208  f1:  0.5583756345177664\n",
      "seaoson:  20112012\n",
      "acc:  0.5056603773584906  f1:  0.5787781350482315\n",
      "seaoson:  20132014\n",
      "acc:  0.5660377358490566  f1:  0.6166666666666667\n",
      "seaoson:  20142015\n",
      "acc:  0.5358490566037736  f1:  0.5844594594594594\n",
      "seaoson:  20152016\n",
      "acc:  0.5283018867924528  f1:  0.5819397993311037\n",
      "seaoson:  20162017\n",
      "acc:  0.5622641509433962  f1:  0.6233766233766233\n",
      "seaoson:  20172018\n",
      "acc:  0.5849387040280211  f1:  0.6403641881638847\n",
      "seaoson:  20182019\n",
      "acc:  0.5481611208406305  f1:  0.6160714285714286\n",
      "seaoson:  20192020\n",
      "acc:  0.5497382198952879  f1:  0.6055045871559633\n",
      "avg acuracy:  0.543846809787357\n",
      " \n"
     ]
    }
   ],
   "source": [
    "##try for ridge regression\n",
    "\n",
    "naive_test_train_regr_models(model = lr, cut_off = 700, regr = True)  ##ok looks like 20162017 is unusually good for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ec6d1b4-0391-432d-b1e3-4c6995a756ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##avg is around 54% for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cbda4b46-47fe-4a7b-858b-0b42d2021fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for  Ridge(alpha=0.001)\n",
      " \n",
      "seaoson:  20082009\n",
      "acc:  0.5351043643263758  f1:  0.6067415730337079\n",
      "seaoson:  20092010\n",
      "acc:  0.5587121212121212  f1:  0.616144975288303\n",
      "seaoson:  20102011\n",
      "acc:  0.5075471698113208  f1:  0.5583756345177664\n",
      "seaoson:  20112012\n",
      "acc:  0.5056603773584906  f1:  0.5787781350482315\n",
      "seaoson:  20132014\n",
      "acc:  0.5660377358490566  f1:  0.6166666666666667\n",
      "seaoson:  20142015\n",
      "acc:  0.5358490566037736  f1:  0.5844594594594594\n",
      "seaoson:  20152016\n",
      "acc:  0.5283018867924528  f1:  0.5819397993311037\n",
      "seaoson:  20162017\n",
      "acc:  0.5622641509433962  f1:  0.6233766233766233\n",
      "seaoson:  20172018\n",
      "acc:  0.5849387040280211  f1:  0.6403641881638847\n",
      "seaoson:  20182019\n",
      "acc:  0.5481611208406305  f1:  0.6160714285714286\n",
      "seaoson:  20192020\n",
      "acc:  0.5497382198952879  f1:  0.6055045871559633\n",
      "avg acuracy:  0.543846809787357\n",
      " \n",
      "results for  RandomForestRegressor(max_depth=3, random_state=0)\n",
      " \n",
      "seaoson:  20082009\n",
      "acc:  0.5009487666034156  f1:  0.6516556291390728\n",
      "seaoson:  20092010\n",
      "acc:  0.5208333333333334  f1:  0.669281045751634\n",
      "seaoson:  20102011\n",
      "acc:  0.4679245283018868  f1:  0.5982905982905983\n",
      "seaoson:  20112012\n",
      "acc:  0.47924528301886793  f1:  0.636842105263158\n",
      "seaoson:  20132014\n",
      "acc:  0.49245283018867925  f1:  0.6237762237762238\n",
      "seaoson:  20142015\n",
      "acc:  0.5037735849056604  f1:  0.628005657708628\n",
      "seaoson:  20152016\n",
      "acc:  0.4849056603773585  f1:  0.6255144032921811\n",
      "seaoson:  20162017\n",
      "acc:  0.5264150943396226  f1:  0.671035386631717\n",
      "seaoson:  20172018\n",
      "acc:  0.5411558669001751  f1:  0.6716791979949874\n",
      "seaoson:  20182019\n",
      "acc:  0.4903677758318739  f1:  0.6481257557436517\n",
      "seaoson:  20192020\n",
      "acc:  0.5366492146596858  f1:  0.6691588785046729\n",
      "avg acuracy:  0.5040610853145964\n",
      " \n",
      "results for  XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
      "             colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "             learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "             scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "             validate_parameters=None, verbosity=None)\n",
      " \n",
      "seaoson:  20082009\n",
      "acc:  0.5256166982922201  f1:  0.5791245791245792\n",
      "seaoson:  20092010\n",
      "acc:  0.5227272727272727  f1:  0.5578947368421053\n",
      "seaoson:  20102011\n",
      "acc:  0.4981132075471698  f1:  0.5128205128205127\n",
      "seaoson:  20112012\n",
      "acc:  0.5264150943396226  f1:  0.5525846702317291\n",
      "seaoson:  20132014\n",
      "acc:  0.5452830188679245  f1:  0.5657657657657658\n",
      "seaoson:  20142015\n",
      "acc:  0.5754716981132075  f1:  0.585635359116022\n",
      "seaoson:  20152016\n",
      "acc:  0.5132075471698113  f1:  0.5239852398523984\n",
      "seaoson:  20162017\n",
      "acc:  0.5528301886792453  f1:  0.6016806722689075\n",
      "seaoson:  20172018\n",
      "acc:  0.5516637478108581  f1:  0.6097560975609757\n",
      "seaoson:  20182019\n",
      "acc:  0.5271453590192644  f1:  0.5588235294117647\n",
      "seaoson:  20192020\n",
      "acc:  0.5890052356020943  f1:  0.5922077922077922\n",
      "avg acuracy:  0.538861733469881\n",
      " \n"
     ]
    }
   ],
   "source": [
    "##now try all regressors\n",
    "\n",
    "for model in regr_models:\n",
    "    naive_test_train_regr_models(model = model, cut_off = 700, regr = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79d7f818-d953-4b00-b845-403952bc2175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for  RidgeClassifier()\n",
      " \n",
      "seaoson:  20082009\n",
      "acc:  0.5199240986717267  f1:  0.5990491283676704\n",
      "seaoson:  20092010\n",
      "acc:  0.5681818181818182  f1:  0.6357827476038338\n",
      "seaoson:  20102011\n",
      "acc:  0.5320754716981132  f1:  0.5782312925170067\n",
      "seaoson:  20112012\n",
      "acc:  0.5377358490566038  f1:  0.6213292117465223\n",
      "seaoson:  20132014\n",
      "acc:  0.5773584905660377  f1:  0.6303630363036303\n",
      "seaoson:  20142015\n",
      "acc:  0.5584905660377358  f1:  0.6151315789473685\n",
      "seaoson:  20152016\n",
      "acc:  0.5509433962264151  f1:  0.6046511627906976\n",
      "seaoson:  20162017\n",
      "acc:  0.5528301886792453  f1:  0.6348228043143298\n",
      "seaoson:  20172018\n",
      "acc:  0.5831873905429071  f1:  0.6609686609686609\n",
      "seaoson:  20182019\n",
      "acc:  0.5691768826619965  f1:  0.6283987915407855\n",
      "seaoson:  20192020\n",
      "acc:  0.5418848167539267  f1:  0.5823389021479713\n",
      "avg acuracy:  0.5537989971887751\n",
      " \n",
      "results for  GaussianNB()\n",
      " \n",
      "seaoson:  20082009\n",
      "acc:  0.4990512333965844  f1:  0.5352112676056339\n",
      "seaoson:  20092010\n",
      "acc:  0.5037878787878788  f1:  0.5544217687074829\n",
      "seaoson:  20102011\n",
      "acc:  0.47735849056603774  f1:  0.484171322160149\n",
      "seaoson:  20112012\n",
      "acc:  0.4981132075471698  f1:  0.5333333333333334\n",
      "seaoson:  20132014\n",
      "acc:  0.539622641509434  f1:  0.5836177474402731\n",
      "seaoson:  20142015\n",
      "acc:  0.5358490566037736  f1:  0.5527272727272727\n",
      "seaoson:  20152016\n",
      "acc:  0.5113207547169811  f1:  0.5247706422018349\n",
      "seaoson:  20162017\n",
      "acc:  0.5169811320754717  f1:  0.5555555555555555\n",
      "seaoson:  20172018\n",
      "acc:  0.5376532399299475  f1:  0.5728155339805824\n",
      "seaoson:  20182019\n",
      "acc:  0.47285464098073554  f1:  0.49411764705882355\n",
      "seaoson:  20192020\n",
      "acc:  0.4869109947643979  f1:  0.5025380710659899\n",
      "avg acuracy:  0.5072275700798557\n",
      " \n",
      "results for  LogisticRegression(random_state=0)\n",
      " \n",
      "seaoson:  20082009\n",
      "acc:  0.5275142314990512  f1:  0.6091051805337518\n",
      "seaoson:  20092010\n",
      "acc:  0.5814393939393939  f1:  0.65086887835703\n",
      "seaoson:  20102011\n",
      "acc:  0.5283018867924528  f1:  0.5777027027027027\n",
      "seaoson:  20112012\n",
      "acc:  0.5377358490566038  f1:  0.6270928462709285\n",
      "seaoson:  20132014\n",
      "acc:  0.5830188679245283  f1:  0.6382978723404255\n",
      "seaoson:  20142015\n",
      "acc:  0.5528301886792453  f1:  0.6121112929623568\n",
      "seaoson:  20152016\n",
      "acc:  0.5509433962264151  f1:  0.6085526315789475\n",
      "seaoson:  20162017\n",
      "acc:  0.5547169811320755  f1:  0.6391437308868502\n",
      "seaoson:  20172018\n",
      "acc:  0.5814360770577933  f1:  0.6638537271448663\n",
      "seaoson:  20182019\n",
      "acc:  0.5691768826619965  f1:  0.6295180722891566\n",
      "seaoson:  20192020\n",
      "acc:  0.5392670157068062  f1:  0.5849056603773585\n",
      "avg acuracy:  0.555125524606942\n",
      " \n",
      "results for  SVC()\n",
      " \n",
      "seaoson:  20082009\n",
      "acc:  0.5199240986717267  f1:  0.6065318818040436\n",
      "seaoson:  20092010\n",
      "acc:  0.5435606060606061  f1:  0.6331811263318112\n",
      "seaoson:  20102011\n",
      "acc:  0.4867924528301887  f1:  0.5374149659863946\n",
      "seaoson:  20112012\n",
      "acc:  0.539622641509434  f1:  0.6336336336336336\n",
      "seaoson:  20132014\n",
      "acc:  0.5830188679245283  f1:  0.6247877758913413\n",
      "seaoson:  20142015\n",
      "acc:  0.5679245283018868  f1:  0.6138279932546374\n",
      "seaoson:  20152016\n",
      "acc:  0.530188679245283  f1:  0.5964343598055106\n",
      "seaoson:  20162017\n",
      "acc:  0.5490566037735849  f1:  0.6384266263237519\n",
      "seaoson:  20172018\n",
      "acc:  0.5901926444833625  f1:  0.6871657754010695\n",
      "seaoson:  20182019\n",
      "acc:  0.5569176882661997  f1:  0.6251851851851851\n",
      "seaoson:  20192020\n",
      "acc:  0.5471204188481675  f1:  0.6004618937644343\n",
      "avg acuracy:  0.5467562936286335\n",
      " \n",
      "results for  RandomForestClassifier(max_depth=3, random_state=0)\n",
      " \n",
      "seaoson:  20082009\n",
      "acc:  0.5692599620493358  f1:  0.715894868585732\n",
      "seaoson:  20092010\n",
      "acc:  0.5587121212121212  f1:  0.6985769728331176\n",
      "seaoson:  20102011\n",
      "acc:  0.5132075471698113  f1:  0.6465753424657534\n",
      "seaoson:  20112012\n",
      "acc:  0.560377358490566  f1:  0.7069182389937108\n",
      "seaoson:  20132014\n",
      "acc:  0.5320754716981132  f1:  0.675392670157068\n",
      "seaoson:  20142015\n",
      "acc:  0.530188679245283  f1:  0.6875784190715183\n",
      "seaoson:  20152016\n",
      "acc:  0.5320754716981132  f1:  0.6787564766839378\n",
      "seaoson:  20162017\n",
      "acc:  0.5509433962264151  f1:  0.6956521739130435\n",
      "seaoson:  20172018\n",
      "acc:  0.5516637478108581  f1:  0.7043879907621247\n",
      "seaoson:  20182019\n",
      "acc:  0.5604203152364273  f1:  0.6889714993804212\n",
      "seaoson:  20192020\n",
      "acc:  0.5366492146596858  f1:  0.6752293577981652\n",
      "avg acuracy:  0.5450521168633391\n",
      " \n",
      "results for  BaggingClassifier()\n",
      " \n",
      "seaoson:  20082009\n",
      "acc:  0.5256166982922201  f1:  0.5644599303135889\n",
      "seaoson:  20092010\n",
      "acc:  0.5189393939393939  f1:  0.5512367491166078\n",
      "seaoson:  20102011\n",
      "acc:  0.4962264150943396  f1:  0.4952741020793951\n",
      "seaoson:  20112012\n",
      "acc:  0.5264150943396226  f1:  0.5649913344887348\n",
      "seaoson:  20132014\n",
      "acc:  0.5415094339622641  f1:  0.5652951699463328\n",
      "seaoson:  20142015\n",
      "acc:  0.5660377358490566  f1:  0.5833333333333334\n",
      "seaoson:  20152016\n",
      "acc:  0.5226415094339623  f1:  0.532347504621072\n",
      "seaoson:  20162017\n",
      "acc:  0.530188679245283  f1:  0.5431192660550458\n",
      "seaoson:  20172018\n",
      "acc:  0.5551663747810858  f1:  0.5889967637540453\n",
      "seaoson:  20182019\n",
      "acc:  0.5096322241681261  f1:  0.5104895104895104\n",
      "seaoson:  20192020\n",
      "acc:  0.5680628272251309  f1:  0.5576407506702412\n",
      "avg acuracy:  0.5327669442118622\n",
      " \n",
      "results for  GradientBoostingClassifier()\n",
      " \n",
      "seaoson:  20082009\n",
      "acc:  0.5332068311195446  f1:  0.6250000000000001\n",
      "seaoson:  20092010\n",
      "acc:  0.5321969696969697  f1:  0.6122448979591837\n",
      "seaoson:  20102011\n",
      "acc:  0.5094339622641509  f1:  0.5695364238410595\n",
      "seaoson:  20112012\n",
      "acc:  0.5320754716981132  f1:  0.6207951070336392\n",
      "seaoson:  20132014\n",
      "acc:  0.5490566037735849  f1:  0.6049586776859504\n",
      "seaoson:  20142015\n",
      "acc:  0.5754716981132075  f1:  0.6376811594202899\n",
      "seaoson:  20152016\n",
      "acc:  0.5358490566037736  f1:  0.5872483221476511\n",
      "seaoson:  20162017\n",
      "acc:  0.5584905660377358  f1:  0.6465256797583081\n",
      "seaoson:  20172018\n",
      "acc:  0.5971978984238179  f1:  0.6973684210526315\n",
      "seaoson:  20182019\n",
      "acc:  0.5359019264448336  f1:  0.609720176730486\n",
      "seaoson:  20192020\n",
      "acc:  0.5445026178010471  f1:  0.5896226415094338\n",
      "avg acuracy:  0.5457621456342526\n",
      " \n",
      "results for  XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "              validate_parameters=None, verbosity=None)\n",
      " \n",
      "[13:18:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "seaoson:  20082009\n",
      "acc:  0.5218216318785579  f1:  0.5813953488372093\n",
      "[13:18:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seaoson:  20092010\n",
      "acc:  0.5492424242424242  f1:  0.6072607260726074\n",
      "[13:18:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "seaoson:  20102011\n",
      "acc:  0.5018867924528302  f1:  0.5432525951557093\n",
      "[13:18:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "seaoson:  20112012\n",
      "acc:  0.5188679245283019  f1:  0.5880452342487884\n",
      "[13:18:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "seaoson:  20132014\n",
      "acc:  0.560377358490566  f1:  0.5989672977624785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:18:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "seaoson:  20142015\n",
      "acc:  0.5811320754716981  f1:  0.6185567010309279\n",
      "[13:18:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "seaoson:  20152016\n",
      "acc:  0.5283018867924528  f1:  0.5748299319727891\n",
      "[13:18:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "seaoson:  20162017\n",
      "acc:  0.5415094339622641  f1:  0.6048780487804878\n",
      "[13:18:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "seaoson:  20172018\n",
      "acc:  0.5936952714535902  f1:  0.6647398843930636\n",
      "[13:18:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "seaoson:  20182019\n",
      "acc:  0.5148861646234676  f1:  0.5568\n",
      "[13:18:27] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "seaoson:  20192020\n",
      "acc:  0.5340314136125655  f1:  0.5572139303482587\n",
      "avg acuracy:  0.5405229434098835\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "##now try all classifiers\n",
    "for model in class_models: \n",
    "    naive_test_train_regr_models(model = model, cut_off = 700, regr = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8320b1b-a272-42e2-b456-fcb51d16ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "##conclusions: some of the average scores are around 55% and some of the top \n",
    "##scores on a season are as high as 58, 59%\n",
    "\n",
    "\n",
    "##next steps: \n",
    "##1. tune the models\n",
    "##2. look into partial_fit across the seaosn for appropriate models that have that"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
