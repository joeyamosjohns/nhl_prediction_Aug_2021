{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c356db71-cc93-49e8-9ad1-4e709f0e1d35",
   "metadata": {},
   "source": [
    "Here we try a different approach based on the work of Leung, discussed here: https://medium.com/coinmonks/4-718-using-machine-learning-to-bet-on-the-nhl-25d16649cd52\n",
    "\n",
    "The main idea is to train on several season (5 or 6) with a traditional test/train split\n",
    "Then, in addition to test scores on those training seasons we check the accuracy on the following two seasons 20162017 and 20172018\n",
    "\n",
    "For this to make sense we use features that do not involve the identity of the teams, ie we do not make dummy variables for each team\n",
    "We keeo track of the difference between home team and away team for things like expecected goals % measured for each team abd averaged \n",
    "for all games up to the game before. We also use the idea of using the difference of home team - away team stats (rather than all stats for home and all stats for away\n",
    "team which would be twoce as many features; this idea seems to have orginated with  Pischedda in 2013.\n",
    "\n",
    "For this preliminary try we use similar features to what Leung used. But I will explore \n",
    "to see what happens when the features are varried; there are a ton of options to explore in this direction.\n",
    "\n",
    "Prelimary results: \n",
    "\n",
    "\n",
    "1.  Classifier models have much improved accuracy over the results so far for model 2 (Pischedda approach): \n",
    "many of the models have accuracy in the \n",
    "\n",
    "58-59% range predicting on 2016-17 and 2017-18 (and f1 scores are solid also)\n",
    "\n",
    "the test performance on the earlier training seasons is even better \n",
    "\n",
    "at 60-61%, \n",
    "\n",
    "\n",
    "which makes sense since the testing on an entire following season after training  and even two seasons after testing \n",
    "is a more stringent test of the models.\n",
    "\n",
    "2. Three regressors are also checked at the bottom and their perfomance is even better.\n",
    "The regressor is fit to predict the goal differential home goals- away goals\n",
    "Then the regression prediction is used to predict home win or loss. This win/loss prediction \n",
    "has:\n",
    "\n",
    "\n",
    "60-61% accuracy \n",
    "\n",
    "for random-forrest and Ridge linear regression( alpha = 0.001)\n",
    "\n",
    "xgboost regressor is only 55% but this is a complex model thaths many hyper-parameters that need\n",
    "tuning so there is room to improve that model. It may turn out to be the top performer when properly tuned (as it often does).\n",
    "\n",
    "\n",
    "\n",
    "Note on tuning: The clasifier models are currently tuned somewhat haphazardly by hand \n",
    "Comparing with the untuned models we can see that all models are not much improved or not improved\n",
    "on their test results in the test/train split on seasons <=2015\n",
    "\n",
    "So the tuning is not having any significant effect in this run.\n",
    "More systematic tuning using random search and grid search, using the training seasons is in order.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103f9139-8e0a-4c45-b3c9-1ef46338c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f4b3a9-23fa-49a2-80c7-a37a2cb9cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "##note KNN or other clusters might be helpful group the teams in smart way ... but not now.\n",
    "#models\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "##regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#classifiers (non-tree)\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDRegressor, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#tree-based classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40becb71-f57a-4d3f-9aaf-c5db75b0871e",
   "metadata": {},
   "source": [
    "\n",
    "##regression models\n",
    "lr = Ridge(alpha=0.001) \n",
    "rfr = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0)\n",
    "svc = SVC()\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=3, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab37356d-c106-4075-97bf-3d59f711822f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10385, 26)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('/Users/joejohns/data_bootcamp/GitHub/nhl_prediction_Aug_2021B/Data/Processed_Data/Approach_3_bulk_Leung_data/data_LJ.csv')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0630de4d-43bf-447b-9e1b-15303735b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['goal_diff_target'] = X['home_goals'] - X['away_goals']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb571dbe-849e-4f12-80fc-3fdb18a747e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'game_id', 'mp_date', 'season', 'home_team', 'away_team',\n",
       "       'home_odds', 'away_odds', 'home_goals', 'away_goals', 'home_win',\n",
       "       'settled_in', 'CF%', 'CSh%', 'CSv%', 'FF%', 'FSh%', 'FSv%', 'GDiff',\n",
       "       'GF%', 'PDO', 'PENDiff', 'SF%', 'SDiff', 'Sh%', 'Sv',\n",
       "       'goal_diff_target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will use this for correlations etc at the bottom\n",
    "\n",
    "data = X.copy()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b597dadb-8ae7-4b86-baed-a1c469daadab",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "X['season'] = X['season'].apply(int)\n",
    "X['game_id'] = X['game_id'].apply(int)\n",
    "X['mp_date'] = X['mp_date'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4474ba8-202e-4f5f-8bc9-0eb07b50e0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9104, 27)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_no_early  = (X['mp_date'].apply(lambda x : x% 10**4) < 900) | (1100 < X['mp_date'].apply(lambda x : x% 10**4))\n",
    "\n",
    "X = X.loc[filt_no_early, : ].copy()  ## keep games < 900 and > 1100\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c3755d9-3c25-4f87-ad9f-a311084e2910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20162017    1060\n",
       "20172018    1052\n",
       "20142015    1046\n",
       "20112012    1037\n",
       "20152016    1036\n",
       "20132014     994\n",
       "20082009     971\n",
       "20102011     962\n",
       "20092010     946\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e8100f8-5d10-43cd-ba9b-b1dbe31af52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = ['Unnamed: 0', 'game_id', 'mp_date', 'season', 'home_team', 'away_team',\n",
    "       'home_odds', 'away_odds', 'home_goals', 'away_goals', 'goal_diff_target', 'home_win',\n",
    "       'settled_in', ]\n",
    "\n",
    "x_cols = ['CF%', 'CSh%', 'CSv%', 'FF%', 'FSh%', 'FSv%', 'GDiff',\n",
    "       'GF%', 'PDO', 'PENDiff', 'SF%', 'SDiff', 'Sh%', 'Sv']\n",
    "columns_to_scale = ['CF%', 'CSh%', 'CSv%', 'FF%', 'FSh%', 'FSv%', 'GDiff',\n",
    "       'GF%', 'PDO', 'PENDiff', 'SF%', 'SDiff', 'Sh%', 'Sv']  ##same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "013eedf8-4f12-4bc8-a544-ee6d9c1c34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = np.array(X.loc[(X['season'] <= 20152016), x_cols].copy())\n",
    "  #features test-train\n",
    "Y = X.loc[(X['season'] <= 20152016), y_cols].copy()\n",
    "   #targets\n",
    "y = np.array(Y['home_win']).reshape(-1,1)\n",
    "\n",
    "\n",
    "              \n",
    "x_16 = X.loc[(X['season'] == 20162017), x_cols].copy()  #features test-train\n",
    "Y_16 = X.loc[(X['season'] == 20162017), y_cols].copy()   #targets\n",
    "y_16 = np.array(Y_16['home_win']).reshape(-1,1)\n",
    "\n",
    "x_17 = X.loc[(X['season'] == 20172018), x_cols].copy()  #features test-train\n",
    "Y_17 = X.loc[(X['season'] == 20172018), y_cols].copy()   #targets\n",
    "y_17 = np.array(Y_17['home_win']).reshape(-1,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f700926e-7aa1-4433-b0f6-9c1d0e14f9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20162017    1060\n",
       "20172018    1052\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] >= 20162017), :]['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efcafef7-cfba-40b5-80ef-46989be16c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20142015    1046\n",
       "20112012    1037\n",
       "20152016    1036\n",
       "20132014     994\n",
       "20082009     971\n",
       "20102011     962\n",
       "20092010     946\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] <= 20152016), :]['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a330af0a-b742-4148-97a9-0ea19ec5d885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6992"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] <= 20152016), :]['season'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6db416ea-49eb-4ce2-a80b-6563c98a329f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9104, 27)\n",
      "(6992, 14) (6992, 13) (6992, 1)\n",
      "(1060, 14) (1060, 1)\n",
      "(1052, 14) (1052, 1)\n",
      "sum of  6992 1060 1052  is:  9104\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(x.shape, Y.shape, y.shape)\n",
    "print(x_16.shape, y_16.shape)\n",
    "print(x_17.shape, y_17.shape)\n",
    "print('sum of ', y.shape[0],y_16.shape[0], y_17.shape[0], ' is: ', y.shape[0] + y_16.shape[0]+y_17.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49d68d84-81e2-4cad-9d42-4915f3b7326b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20142015    1046\n",
       "20112012    1037\n",
       "20152016    1036\n",
       "20132014     994\n",
       "20082009     971\n",
       "20102011     962\n",
       "20092010     946\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbe491e0-60db-4f93-8468-d0b795a26594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20162017    1060\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_16['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "906530c2-e1f4-48c4-bfd4-47fc5e1758a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20172018    1052\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_17['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c06b6ab7-34a9-49d7-a7e2-f409c6d58675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "std_scal = StandardScaler()\n",
    "mm_scal = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab763f-9577-40d1-8600-787c5b9cce2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17a41bcb-2dd9-4ff2-921e-911de2ecf2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC() TEST:  0.5961401000714797\n",
      "RidgeClassif TEST:  0.6025732666190136\n",
      "RandomForest TEST:  0.5654038598999285\n",
      "GaussianNB() TEST:  0.5954253037884203\n",
      "LogisticRegr TEST:  0.6025732666190136\n",
      "BaggingClass TEST:  0.5160829163688349\n",
      "GradientBoos TEST:  0.6111508220157256\n",
      "[18:50:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifie TEST:  0.544674767691208\n"
     ]
    }
   ],
   "source": [
    "###NOT tuned \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=314, stratify=y)\n",
    "\n",
    "#do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "x_train_sc = std_scal.fit_transform(x_train)\n",
    "    \n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "##classifier models\n",
    "lrc2 = RidgeClassifier(alpha =0.001)\n",
    "gnb2 = GaussianNB()\n",
    "lgr2 = LogisticRegression(random_state = 0, max_iter = 500)\n",
    "svc2 = SVC(kernel = 'rbf')\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc2 =  RandomForestClassifier( random_state=0)\n",
    "bc2 = BaggingClassifier()\n",
    "gbc2 = GradientBoostingClassifier()\n",
    "xgbc2 = XGBClassifier()\n",
    "##quick checks \n",
    "for model in [svc2, lrc2, rfc2, gnb2, lgr2, bc2, gbc2, xgbc2]:\n",
    "    model.fit(x_train_sc, y_train.ravel())\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    y_predt= model.predict(x_train_sc)  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "   # f1 = f1_score(y_test,y_pred)\n",
    "    acct = accuracy_score(y_train, y_predt)\n",
    "    #f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], 'TEST: ', acc ) #, f1, 'training : ', acct, f1t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc8d4bab-b709-4788-8603-fe92b9c9bba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10) TEST:  0.6018584703359543 0.6944596818431157 training :  0.6184516359735384 0.7056551724137932\n",
      "RidgeClassif TEST:  0.6047176554681916 0.6983087834151664 training :  0.5678526729840873 0.667948894078857\n",
      "RandomForest TEST:  0.5868477483917084 0.6685779816513763 training :  0.8342571070981584 0.8634958032690326\n",
      "GaussianNB() TEST:  0.5954253037884203 0.6310299869621905 training :  0.5571249776506347 0.5914563747319809\n",
      "LogisticRegr TEST:  0.6040028591851322 0.6972677595628415 training :  0.5683890577507599 0.6679504814305365\n",
      "BaggingClass TEST:  0.5511079342387419 0.6089663760896638 training :  0.9998212050777758 0.9998372660699756\n",
      "GradientBoos TEST:  0.6061472480343102 0.690275435637999 training :  0.6660110852851779 0.7351857102353275\n",
      "[18:51:04] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifie TEST:  0.5875625446747677 0.6662810873337188 training :  0.7216163060969069 0.7750975010833455\n"
     ]
    }
   ],
   "source": [
    "##TUNED! By hand ... improving test score on on seasons <= 2015\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=314, stratify=y)\n",
    "\n",
    "#do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "x_train_sc = std_scal.fit_transform(x_train)\n",
    "    \n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "##classifier models\n",
    "lrc = RidgeClassifier(alpha =0.2)\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0, C =10, max_iter = 500)\n",
    "svc = SVC(kernel = 'rbf', C =10)\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=10, random_state=0, n_estimators = 40)\n",
    "bc = BaggingClassifier(n_estimators  = 60, max_samples = 0.85)\n",
    "gbc = GradientBoostingClassifier(learning_rate =0.1, n_estimators = 40, max_depth =4 )\n",
    "xgbc = XGBClassifier(n_estimators= 45, eta=0.05)\n",
    "\n",
    "##quick checks \n",
    "for model in [svc, lrc, rfc, gnb, lgr, bc, gbc, xgbc]:\n",
    "    model.fit(x_train_sc, y_train.ravel())\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    y_predt= model.predict(x_train_sc)  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    acct = accuracy_score(y_train, y_predt)\n",
    "    f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], 'TEST: ', acc, f1, 'training : ', acct, f1t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2520644a-d552-4517-9e67-409a7569dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10) Tuned model TEST results on 2016-17:  0.6017110266159695 0.7071977638015374\n",
      "RidgeClassif Tuned model TEST results on 2016-17:  0.6055133079847909 0.7205387205387206\n",
      "RandomForest Tuned model TEST results on 2016-17:  0.5817490494296578 0.6774193548387096\n",
      "GaussianNB() Tuned model TEST results on 2016-17:  0.5712927756653993 0.6232247284878863\n",
      "LogisticRegr Tuned model TEST results on 2016-17:  0.6036121673003803 0.7188132164531355\n",
      "BaggingClass Tuned model TEST results on 2016-17:  0.5456273764258555 0.6218354430379748\n",
      "GradientBoos Tuned model TEST results on 2016-17:  0.5846007604562737 0.6867383512544804\n",
      "XGBClassifie Tuned model TEST results on 2016-17:  0.5893536121673004 0.6795252225519289\n"
     ]
    }
   ],
   "source": [
    "x_17_sc = std_scal.transform(x_17)\n",
    "x_test2 = x_17_sc.copy()\n",
    "y_test2 = y_17.copy()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [svc, lrc, rfc, gnb, lgr, bc, gbc, xgbc]:\n",
    "    #model.fit(x_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "   \n",
    "    #acct = accuracy_score(y_train, y_predt)\n",
    "    #f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], 'Tuned model TEST results on 2016-17: ', acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6c08ae9-2216-437f-8041-4b3c32ae23d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC() NOT Tuned model TEST results on 2016-17:  0.5979087452471483 0.7128309572301427\n",
      "RidgeClassif NOT Tuned model TEST results on 2016-17:  0.6074144486692015 0.7215104517869183\n",
      "RandomForest NOT Tuned model TEST results on 2016-17:  0.5247148288973384 0.6081504702194357\n",
      "GaussianNB() NOT Tuned model TEST results on 2016-17:  0.5712927756653993 0.6232247284878863\n",
      "LogisticRegr NOT Tuned model TEST results on 2016-17:  0.6017110266159695 0.7163168584969533\n",
      "BaggingClass NOT Tuned model TEST results on 2016-17:  0.49619771863117873 0.5250896057347669\n",
      "GradientBoos NOT Tuned model TEST results on 2016-17:  0.5903041825095057 0.6928011404133999\n",
      "XGBClassifie NOT Tuned model TEST results on 2016-17:  0.5532319391634981 0.6310832025117739\n"
     ]
    }
   ],
   "source": [
    "x_17_sc = std_scal.transform(x_17)\n",
    "x_test2 = x_17_sc.copy()\n",
    "y_test2 = y_17.copy()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [svc2, lrc2, rfc2, gnb2, lgr2, bc2, gbc2, xgbc2]:\n",
    "    #model.fit(x_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "   \n",
    "    #acct = accuracy_score(y_train, y_predt)\n",
    "    #f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], 'NOT Tuned model TEST results on 2016-17: ', acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4eb0e-fb90-47f7-b8de-76f2ad603de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "846fbbe9-3db1-4153-8b38-821967496586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC() NOT Tuned model TEST results on 20150-16  0.5886792452830188 0.6872309899569583 training :  0.7216163060969069 0.7750975010833455\n",
      "RidgeClassif NOT Tuned model TEST results on 20150-16  0.5877358490566038 0.6880799428979301 training :  0.7216163060969069 0.7750975010833455\n",
      "RandomForest NOT Tuned model TEST results on 20150-16  0.539622641509434 0.6032520325203251 training :  0.7216163060969069 0.7750975010833455\n",
      "GaussianNB() NOT Tuned model TEST results on 20150-16  0.5726415094339623 0.6050566695727986 training :  0.7216163060969069 0.7750975010833455\n",
      "LogisticRegr NOT Tuned model TEST results on 20150-16  0.5886792452830188 0.6867816091954023 training :  0.7216163060969069 0.7750975010833455\n",
      "BaggingClass NOT Tuned model TEST results on 20150-16  0.5481132075471699 0.5633546034639927 training :  0.7216163060969069 0.7750975010833455\n",
      "GradientBoos NOT Tuned model TEST results on 20150-16  0.5830188679245283 0.6666666666666666 training :  0.7216163060969069 0.7750975010833455\n",
      "XGBClassifie NOT Tuned model TEST results on 20150-16  0.530188679245283 0.6003210272873195 training :  0.7216163060969069 0.7750975010833455\n"
     ]
    }
   ],
   "source": [
    "x_16_sc = std_scal.transform(x_16)\n",
    "x_test2 = x_16_sc.copy()\n",
    "y_test2 = y_16.copy()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [svc2, lrc2, rfc2, gnb2, lgr2, bc2, gbc2, xgbc2]:\n",
    "    #model.fit(x_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "   \n",
    "    #acct = accuracy_score(y_train, y_predt)\n",
    "    #f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], 'NOT Tuned model TEST results on 20150-16 ', acc, f1, 'training : ', acct, f1t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfce046b-106f-42c4-9cdb-6d5c19e01495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10) NOT Tuned model TEST results on 20150-16  0.5820754716981132 0.6735445836403832 training :  0.7216163060969069 0.7750975010833455\n",
      "RidgeClassif NOT Tuned model TEST results on 20150-16  0.5867924528301887 0.6866952789699571 training :  0.7216163060969069 0.7750975010833455\n",
      "RandomForest NOT Tuned model TEST results on 20150-16  0.5433962264150943 0.6327769347496207 training :  0.7216163060969069 0.7750975010833455\n",
      "GaussianNB() NOT Tuned model TEST results on 20150-16  0.5726415094339623 0.6050566695727986 training :  0.7216163060969069 0.7750975010833455\n",
      "LogisticRegr NOT Tuned model TEST results on 20150-16  0.5877358490566038 0.6862885857860732 training :  0.7216163060969069 0.7750975010833455\n",
      "BaggingClass NOT Tuned model TEST results on 20150-16  0.5377358490566038 0.5996732026143792 training :  0.7216163060969069 0.7750975010833455\n",
      "GradientBoos NOT Tuned model TEST results on 20150-16  0.589622641509434 0.6746447270007478 training :  0.7216163060969069 0.7750975010833455\n",
      "XGBClassifie NOT Tuned model TEST results on 20150-16  0.5830188679245283 0.6610429447852761 training :  0.7216163060969069 0.7750975010833455\n"
     ]
    }
   ],
   "source": [
    "x_16_sc = std_scal.transform(x_16)\n",
    "x_test2 = x_16_sc.copy()\n",
    "y_test2 = y_16.copy()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [svc, lrc, rfc, gnb, lgr, bc, gbc, xgbc]:\n",
    "    #model.fit(x_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "   \n",
    "    #acct = accuracy_score(y_train, y_predt)\n",
    "    #f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12],  'NOT Tuned model TEST results on 20150-16 ', acc, f1, 'training : ', acct, f1t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d590d1-2010-443d-8049-2391e26655cd",
   "metadata": {},
   "source": [
    "Now try regression models on goal differential ... converted to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbe187d0-bfab-4187-b05f-1c2ce4864680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_win(x):\n",
    "    if x>=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "v_win = np.vectorize(make_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0071d851-329d-4e7e-a579-b43d03a1d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51690533 0.36376034 0.18694447 0.27135373 0.50487017]\n",
      "Ridge(alpha=50000) TEST:  0.6197283774124375 0.7622877569258267 training :  0.619881995351332 0.7614452423698383\n",
      "[0.85707395 0.43331879 0.02921838 0.14527683 0.87678157]\n",
      "RandomForestRegresso TEST:  0.5782701929949964 0.6875 training :  0.6241730734847131 0.7214418234826397\n",
      "[ 0.32998556  0.18154603 -0.6979711  -0.42923597 -0.45748696]\n",
      "XGBRegressor(base_sc TEST:  0.5518227305218013 0.6369426751592356 training :  0.8975505095655283 0.9153493869109175\n"
     ]
    }
   ],
   "source": [
    "##regression models now ...\n",
    "\n",
    "y = Y['goal_diff_target'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "x_train_sc = std_scal.fit_transform(x_train)\n",
    "    \n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "\n",
    "\n",
    "lr = Ridge(alpha=50000) \n",
    "rfr = RandomForestRegressor(max_depth=4, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [lr, rfr, xgbr]:\n",
    "    model.fit(x_train_sc, y_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    print(y_pred[0:5])\n",
    "    y_predw = v_win(y_pred)\n",
    "    y_predt= model.predict(x_train_sc)  \n",
    "    y_predwt = v_win(y_predt) \n",
    "    y_trainw = v_win(y_train)\n",
    "    y_testw = v_win(y_test)  #same as usual win/loss\n",
    "    acc = accuracy_score(y_testw, y_predw)\n",
    "    f1 = f1_score(y_testw,y_predw)\n",
    "    acct = accuracy_score(y_trainw, y_predwt)\n",
    "    f1t = f1_score(y_trainw,y_predwt)\n",
    "    \n",
    "  \n",
    "    print(str(model)[0:20], 'TEST: ', acc, f1 ,'training : ', acct, f1t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7f614b4-4a1e-44a1-9c1e-4af609d903a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68702936 0.35889007 0.40138433 0.30298112 0.18451694]\n",
      "Ridge(alpha= TEST:  0.5962264150943396 0.7440191387559809\n",
      "[0.53925406 0.59556936 0.70076785 0.33123691 0.09648971]\n",
      "RandomForest TEST:  0.6103773584905661 0.7109867039888034\n",
      "[-0.4977314   0.12039401  1.1607935   0.45659927 -0.72441846]\n",
      "XGBRegressor TEST:  0.5490566037735849 0.6294573643410853\n"
     ]
    }
   ],
   "source": [
    "##regression models now ...\n",
    "x_test = x_16.copy()\n",
    "y_test = Y_16['goal_diff_target'].copy()\n",
    "\n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [lr, rfr, xgbr]:\n",
    "   \n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    print(y_pred[0:5])\n",
    "    y_predw = v_win(y_pred)\n",
    "   \n",
    "    y_testw = v_win(y_test)  #same as usual win/loss\n",
    "    \n",
    "    acc = accuracy_score(y_testw, y_predw)\n",
    "    f1 = f1_score(y_testw,y_predw)\n",
    "    \n",
    "  \n",
    "    print(str(model)[0:12], 'TEST: ', acc, f1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "979933b5-81e5-41fc-9eff-895bb6406d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1986591  0.15493199 0.40366176 0.50203642 0.08442206]\n",
      "Ridge(alpha= TEST:  0.5998098859315589 0.7495538370017846\n",
      "[-0.04304532 -0.10187141  0.3700241   0.56126893 -0.23675489]\n",
      "RandomForest TEST:  0.6017110266159695 0.7104353835521771\n",
      "[ 0.5456671   0.07267933 -0.39206013  1.918616    0.21196198]\n",
      "XGBRegressor TEST:  0.55893536121673 0.6419753086419753\n"
     ]
    }
   ],
   "source": [
    "##regression models now ...\n",
    "x_test = x_17.copy()\n",
    "y_test = Y_17['goal_diff_target'].copy()\n",
    "\n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [lr, rfr, xgbr]:\n",
    "   \n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    print(y_pred[0:5])\n",
    "    y_predw = v_win(y_pred)\n",
    "   \n",
    "    y_testw = v_win(y_test)  #same as usual win/loss\n",
    "    \n",
    "    acc = accuracy_score(y_testw, y_predw)\n",
    "    f1 = f1_score(y_testw,y_predw)\n",
    "    \n",
    "  \n",
    "    print(str(model)[0:12], 'TEST: ', acc, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3db8c-155b-46d6-8b5f-f355b49c4ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
